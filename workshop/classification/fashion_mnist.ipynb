{"cells":[{"cell_type":"markdown","source":["# License"],"metadata":{"id":"zDXKczjhQuR2"},"id":"zDXKczjhQuR2"},{"cell_type":"code","execution_count":null,"id":"4deef71b-058f-401d-9a00-366910f66fc6","metadata":{"id":"4deef71b-058f-401d-9a00-366910f66fc6"},"outputs":[],"source":["# Copyright 2021 University of San Andres' Authors."]},{"cell_type":"code","execution_count":null,"id":"29caa66b-a99a-4191-b871-78bb20d7b62b","metadata":{"id":"29caa66b-a99a-4191-b871-78bb20d7b62b"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"code","execution_count":null,"id":"ff84ed0c-f263-4f2c-be83-fe067661a54a","metadata":{"id":"ff84ed0c-f263-4f2c-be83-fe067661a54a"},"outputs":[],"source":["#@title MIT License\n","#\n","# Copyright (c) 2021 University of San Andres\n","#\n","# Permission is hereby granted, free of charge, to any person obtaining a\n","# copy of this software and associated documentation files (the \"Software\"),\n","# to deal in the Software without restriction, including without limitation\n","# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n","# and/or sell copies of the Software, and to permit persons to whom the\n","# Software is furnished to do so, subject to the following conditions:\n","#\n","# The above copyright notice and this permission notice shall be included in\n","# all copies or substantial portions of the Software.\n","#\n","# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n","# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n","# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n","# DEALINGS IN THE SOFTWARE."]},{"cell_type":"markdown","id":"4b941f0c-2c70-47e7-aeda-9e046a81c6fc","metadata":{"id":"4b941f0c-2c70-47e7-aeda-9e046a81c6fc"},"source":["# Clasificación Fashion MNIST"]},{"cell_type":"markdown","id":"1984f5d0-4e45-4cb2-9f55-0bb418b9d806","metadata":{"id":"1984f5d0-4e45-4cb2-9f55-0bb418b9d806"},"source":["En esta primera instancia, vamos a declarar o importar las dependencias que necesitamos para comenzar a trabajar con el tutorial.\n","\n","A través de las palabras reservadas de Python3, como por ejemplo `import`, inicializaremos los siguientes paquetes."]},{"cell_type":"code","execution_count":null,"id":"a97e98ee-fbe1-4640-9c9f-d95fd6d6591a","metadata":{"id":"a97e98ee-fbe1-4640-9c9f-d95fd6d6591a"},"outputs":[],"source":["%load_ext tensorboard\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(tf.__version__)"]},{"cell_type":"markdown","source":["Luego, verificamos que tengamos correctamente instalada la placa de video, o mejor dicho, la *Graphical Processing Unit* (*GPU*)."],"metadata":{"id":"JIpFTcDhQ14a"},"id":"JIpFTcDhQ14a"},{"cell_type":"code","execution_count":null,"id":"26ab5752-39e6-46eb-9410-e798d396d9fb","metadata":{"id":"26ab5752-39e6-46eb-9410-e798d396d9fb"},"outputs":[],"source":["gpu_devices = tf.config.list_physical_devices('GPU')\n","for device in gpu_devices:\n","    tf.config.experimental.set_memory_growth(device, True)\n","\n","print('Num GPU available:', len(gpu_devices))"]},{"cell_type":"markdown","id":"7f026531-470f-43c7-b00e-a6cfc83168b4","metadata":{"id":"7f026531-470f-43c7-b00e-a6cfc83168b4"},"source":["## Descargá el set de datos Fashion MNIST"]},{"cell_type":"markdown","id":"78a83d26-e30c-412a-b307-bc90a2600072","metadata":{"id":"78a83d26-e30c-412a-b307-bc90a2600072"},"source":["Este tutorial usa el conjunto de datos más famoso conocido como [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist). Éste contiene 70.000 imágenes de distintas prendas de vestir que pertenecen a alguna de las 10 categorías. Cada imagen son de baja resolución (28x28 pixels) como se ve aquí:\n","\n","<table>\n","  <tr><td align=\"center\">\n","    <img src=\"https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png\"\n","         alt=\"Fashion MNIST Sprite\" width=\"1024\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>Figura 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Muestras Fashion MNIST</a><br/>&nbsp;\n","  </td></tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"id":"9b9c0aae-76b9-4198-9281-5aa4b46349a2","metadata":{"id":"9b9c0aae-76b9-4198-9281-5aa4b46349a2"},"outputs":[],"source":["(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"]},{"cell_type":"markdown","id":"f31e7d6b-1530-4a44-9b9f-a6fea2daa7bc","metadata":{"id":"f31e7d6b-1530-4a44-9b9f-a6fea2daa7bc"},"source":["Al ejecutar el código de la celda anterior, devuelven los datos en las variables:\n","\n","- `train_images` y `train_labels` que se utilizarán en el momento del entrenamiento de la red neuronal.\n","- `test_images` y `test_labels` serán utilizadas a la hora de evaluar el model entrenado."]},{"cell_type":"markdown","id":"170a3303-aa6f-4e97-8fe6-521970b409f8","metadata":{"id":"170a3303-aa6f-4e97-8fe6-521970b409f8"},"source":["Las etiquetas recolectadas por lo creadores de los datos corresponden a la siguiente tabla:\n","\n","| Label | Class       |\n","|-------|-------------|\n","| 0     | T-shirt/top |\n","| 1     | Trouser     |\n","| 2     | Pullover    |\n","| 3     | Dress       |\n","| 4     | Coat        |\n","| 5     | Sandal      |\n","| 6     | Shirt       |\n","| 7     | Sneaker     |\n","| 8     | Bag         |\n","| 9     | Ankle boot  |\n","\n","Las imágenes son vectores Numpy de 28x28, con valores de píxeles desde 0 hasta 255. Las etiquetas son vectores de enteros, desde el 0 al 9."]},{"cell_type":"code","execution_count":null,"id":"8e704382-1aaf-44d8-81f8-7d73555e8dc8","metadata":{"id":"8e704382-1aaf-44d8-81f8-7d73555e8dc8"},"outputs":[],"source":["class_names = [\n","    'T-shirt/top', \n","    'Trouser', \n","    'Pullover', \n","    'Dress', \n","    'Coat',\n","    'Sandal', \n","    'Shirt',\n","    'Sneaker', \n","    'Bag', \n","    'Ankle boot'\n","]"]},{"cell_type":"markdown","id":"b7720a23-28aa-45b9-af3d-654dd3bb5fe7","metadata":{"id":"b7720a23-28aa-45b9-af3d-654dd3bb5fe7"},"source":["## Explorando los datos\n","\n","Es muy importante saber con qué datos estamos trabajando. Esto significa saber qué tamaño tienen los datos, las dimensiones de las imágenes, así como también saber cuántos datos tenemos para entrenar, y cuántos datos tenemos para hacer las pruebas de la red, una vez entrenada."]},{"cell_type":"markdown","source":["Lo primero es visualizar el tamaño del set de entrenamiento. Para eso ejecutamos la siguiente celda."],"metadata":{"id":"AzCG5jBRRxDT"},"id":"AzCG5jBRRxDT"},{"cell_type":"code","execution_count":null,"id":"be8077df-afdf-4f04-9ed4-69eb33361a8b","metadata":{"id":"be8077df-afdf-4f04-9ed4-69eb33361a8b"},"outputs":[],"source":["train_images.shape"]},{"cell_type":"markdown","source":["Vemos que existen 60000 imágenes, donde cada imágen tiene 28 de ancho y 28 de alto, tal y como habíamos dicho en un principio.\n","\n","Luego, si tenemos la cantidad de etiquetas suficiente para las imágenes."],"metadata":{"id":"INNVJVDQR3bs"},"id":"INNVJVDQR3bs"},{"cell_type":"code","execution_count":null,"id":"aa7b5d1b-367c-46e4-9a22-407763475ae3","metadata":{"id":"aa7b5d1b-367c-46e4-9a22-407763475ae3"},"outputs":[],"source":["len(train_labels)"]},{"cell_type":"markdown","source":["Y vemos que efectivamente tenemos una etiqueta por cada imagen.\n","\n","Del mismo modo hacemos para el set de prueba."],"metadata":{"id":"g2hogI2pSFaB"},"id":"g2hogI2pSFaB"},{"cell_type":"code","execution_count":null,"id":"e01ff3c2-76ce-4c63-aaf8-acaa543b8dec","metadata":{"id":"e01ff3c2-76ce-4c63-aaf8-acaa543b8dec"},"outputs":[],"source":["test_images.shape"]},{"cell_type":"markdown","source":["Tenemos 10.000 imágenes, donde cada una es de 28x28."],"metadata":{"id":"2Du2m7ZsSQG2"},"id":"2Du2m7ZsSQG2"},{"cell_type":"code","execution_count":null,"id":"190a18a6-3ca7-4805-90be-4d664847c567","metadata":{"id":"190a18a6-3ca7-4805-90be-4d664847c567"},"outputs":[],"source":["len(test_labels)"]},{"cell_type":"markdown","source":["Y también tenemos la cantidad de etiquetas correcta para este otro set de datos."],"metadata":{"id":"mtjHG1kfSUZY"},"id":"mtjHG1kfSUZY"},{"cell_type":"markdown","id":"9e893ef3-1000-4635-ad07-fea05ecd6528","metadata":{"id":"9e893ef3-1000-4635-ad07-fea05ecd6528"},"source":["## Preprocesando los datos\n","\n","Al igual que muchos otros set de datos, es importante aplicar transformaciones o un preprocesamiento de tal modo que la red neuronal tenga menos trabajo por hacer y le sea más fácil."]},{"cell_type":"code","execution_count":null,"id":"a8387c3b-a1c6-4a25-8183-e9753ce47bf9","metadata":{"id":"a8387c3b-a1c6-4a25-8183-e9753ce47bf9"},"outputs":[],"source":["plt.figure()\n","plt.imshow(train_images[0])\n","plt.colorbar()\n","plt.grid(False)\n","plt.show()"]},{"cell_type":"markdown","source":["Se ve que la imagen tiene valores en sus pixels que van desde 0 a 255. Una gran ayuda para la red neuronal es normalizar los valores entre 0 y 1. Para ello dividimos cada uno de los pixels por `255.0`."],"metadata":{"id":"FLzQ3Jv9Sp0k"},"id":"FLzQ3Jv9Sp0k"},{"cell_type":"code","execution_count":null,"id":"04f58019-6fc9-45fa-9510-85822535f727","metadata":{"id":"04f58019-6fc9-45fa-9510-85822535f727"},"outputs":[],"source":["train_images = train_images / 255.0\n","\n","test_images = test_images / 255.\n","\n","# Show the figures\n","plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(train_images[i])\n","    plt.xlabel(class_names[train_labels[i]])\n","plt.show()"]},{"cell_type":"markdown","id":"612d1023-d261-4170-a352-9a77465f47ef","metadata":{"id":"612d1023-d261-4170-a352-9a77465f47ef"},"source":["## Construyendo el modelo\n","\n","Lo siguiente, y una de las partes claves del entrenamiento es definir la arquitectura de la red neuronal. Para eso Tensorflow nos ayudará para que sea fácil describir tal arquitecutra.\n","\n","Utilizaremos una arquitectura de profundidad 3, es decir, que tiene 3 capas. En este caso podemos usar las siguientes:\n","\n","* `tf.keras.layers.Flatten(input_shape=(28, 28))`\n","* `tf.keras.layers.Dense(128, activation='relu')`\n","* `tf.keras.layers.Dense(10)`\n","\n","A la primer capa le vamos a poner `input_shape` de tal modo que sepamos el tamaño que haya que aplicarle a la entrada de la red. En este caso, serán imágenes de `28x28`. Las otras dos capas van a ser de tipo `Dense`, es decir, donde a cada una le especificamos la cantidad de neuronas que van a tener. La segunda es de 128 y la última de 10.\n","\n","**IMPORTANTE**: Pueden probar otro tipo de capas como las siguientes si se animan. Sino pueden ver otro de nuestros tutoriales con estas capas. \n","\n","Para más capas seguir este link: https://www.tensorflow.org/api_docs/python/tf/keras/layers"]},{"cell_type":"code","execution_count":null,"id":"46c5b6d0-be1b-4f84-9d61-b0bf82522add","metadata":{"id":"46c5b6d0-be1b-4f84-9d61-b0bf82522add"},"outputs":[],"source":["model = tf.keras.Sequential([\n","    # Complete the layers\n","])"]},{"cell_type":"code","execution_count":null,"id":"5c837fed-01b3-4d4d-b57c-2798d112f2bd","metadata":{"id":"5c837fed-01b3-4d4d-b57c-2798d112f2bd"},"outputs":[],"source":["model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","id":"d6b86186-fee0-4aac-8549-a6502e5cc77a","metadata":{"id":"d6b86186-fee0-4aac-8549-a6502e5cc77a"},"source":["# Entrenamiento\n","\n","Para entrenar, primero vamos a inicializar Tensorboard para que nos muestre el entrenamiento de una manera más entrenida. Y luego, ponemos a entrenar nuestra red neuronal."]},{"cell_type":"code","execution_count":null,"id":"d5bf1304-7955-4351-8aab-77b81731e2f2","metadata":{"id":"d5bf1304-7955-4351-8aab-77b81731e2f2"},"outputs":[],"source":["import os\n","import datetime\n","\n","basedir = '/tmp/fashion/'\n","logdir = os.path.join(basedir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","os.makedirs(logdir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"id":"f300b639-1fb4-4105-a74e-0d0bde86c424","metadata":{"id":"f300b639-1fb4-4105-a74e-0d0bde86c424"},"outputs":[],"source":["%tensorboard --reload_multifile True --logdir {basedir}"]},{"cell_type":"code","execution_count":null,"id":"d7b8861b-dd8c-4b6c-8c55-c0604069bb4f","metadata":{"id":"d7b8861b-dd8c-4b6c-8c55-c0604069bb4f"},"outputs":[],"source":["tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","model.fit(train_images, train_labels, epochs=1, callbacks=[tensorboard_callback])"]},{"cell_type":"markdown","id":"e89718d5-90a1-4831-a860-f4d81d9040f1","metadata":{"id":"e89718d5-90a1-4831-a860-f4d81d9040f1"},"source":["### Evaluar precisión\n","\n","Una vez entrenado, hay que ver cómo se porta con información o imágenes nunca antes vistas. ¡Veamos!"]},{"cell_type":"code","execution_count":null,"id":"1edb669a-c287-4790-845c-d555e54d88fe","metadata":{"id":"1edb669a-c287-4790-845c-d555e54d88fe"},"outputs":[],"source":["test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n","print(f'\\nTest accuracy: {test_acc * 100:.2f}%')"]},{"cell_type":"markdown","id":"776b5ad5-cbea-47d3-ae5c-57036f71b6e9","metadata":{"id":"776b5ad5-cbea-47d3-ae5c-57036f71b6e9"},"source":["### Hacer predicciones\n","\n","También podemos hacer predicciones e imprimir exactamente lo que está intentando clasificar la red. Para eso vamos a generar un modelo nuevo con un capa adicional al final llamada `Softmax`. Y al ejecutar la siguiente celda, vamos a ver que la red neuronal está diciendo que para una imagen, existe posibilidades de que sea una de las opciones. Pero les asigna un valor (una probabilidad) a cada clase de tal modo que el valor más grande es con el que nos vamos a quedar."]},{"cell_type":"code","execution_count":null,"id":"c41351e4-b487-4597-b1fb-b3c3d76df235","metadata":{"id":"c41351e4-b487-4597-b1fb-b3c3d76df235"},"outputs":[],"source":["probability_model = tf.keras.Sequential([\n","    model,\n","    tf.keras.layers.Softmax(),\n","])\n","\n","predictions = probability_model.predict(test_images)\n","\n","predictions[0]"]},{"cell_type":"markdown","source":["En la siguiente celda nos quedamos con la predicción más grande."],"metadata":{"id":"AK-OXySUWp2U"},"id":"AK-OXySUWp2U"},{"cell_type":"code","execution_count":null,"id":"2ff15bfc-6495-4f3c-9c14-c93a1dcd2506","metadata":{"id":"2ff15bfc-6495-4f3c-9c14-c93a1dcd2506"},"outputs":[],"source":["np.argmax(predictions[0])"]},{"cell_type":"markdown","source":["Y verificamos que la etiqueta verdadera, ¡también es un 9!"],"metadata":{"id":"JNYljxBaWuXJ"},"id":"JNYljxBaWuXJ"},{"cell_type":"code","execution_count":null,"id":"05fb7a70-d5bb-4aed-8c75-17e5518a9e6e","metadata":{"id":"05fb7a70-d5bb-4aed-8c75-17e5518a9e6e"},"outputs":[],"source":["test_labels[0]"]},{"cell_type":"markdown","id":"45f7248b-2ac2-4aca-b6cb-167e6f2095d6","metadata":{"id":"45f7248b-2ac2-4aca-b6cb-167e6f2095d6"},"source":["### Visualizar y verificar más predicciones\n","\n","Ahora vamos verificar las predicciones de una manera más visual.\n","\n","Por favor, ejecuten la siguiente celda para seguir avanzando.\n"]},{"cell_type":"code","source":["def plot_image(i, predictions_array, true_label, img):\n","  true_label, img = true_label[i], img[i]\n","  plt.grid(False)\n","  plt.xticks([])\n","  plt.yticks([])\n","\n","  plt.imshow(img, cmap=plt.cm.binary)\n","\n","  predicted_label = np.argmax(predictions_array)\n","  if predicted_label == true_label:\n","    color = 'blue'\n","  else:\n","    color = 'red'\n","\n","  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n","                                100*np.max(predictions_array),\n","                                class_names[true_label]),\n","                                color=color)\n","\n","def plot_value_array(i, predictions_array, true_label):\n","  true_label = true_label[i]\n","  plt.grid(False)\n","  plt.xticks(range(10))\n","  plt.yticks([])\n","  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n","  plt.ylim([0, 1])\n","  predicted_label = np.argmax(predictions_array)\n","\n","  thisplot[predicted_label].set_color('red')\n","  thisplot[true_label].set_color('blue')"],"metadata":{"id":"44rHyquPXK5H"},"id":"44rHyquPXK5H","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Veamos la imagen, la predicción y la probabilidad de clasificación, todo en uno."],"metadata":{"id":"yZHFl-aZXUsE"},"id":"yZHFl-aZXUsE"},{"cell_type":"code","execution_count":null,"id":"10ac6a43-d96e-43f7-8406-86ee21cea3be","metadata":{"id":"10ac6a43-d96e-43f7-8406-86ee21cea3be"},"outputs":[],"source":["i = 0\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plot_image(i, predictions[i], test_labels, test_images)\n","plt.subplot(1,2,2)\n","plot_value_array(i, predictions[i],  test_labels)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"f5aa2b75-a1c7-40a3-bd94-ffc303a5586f","metadata":{"id":"f5aa2b75-a1c7-40a3-bd94-ffc303a5586f"},"outputs":[],"source":["i = 12\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plot_image(i, predictions[i], test_labels, test_images)\n","plt.subplot(1,2,2)\n","plot_value_array(i, predictions[i],  test_labels)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"edd31cc9-816b-472a-8180-dcd519d9d261","metadata":{"id":"edd31cc9-816b-472a-8180-dcd519d9d261"},"outputs":[],"source":["# Plot the first X test images, their predicted labels, and the true labels.\n","# Color correct predictions in blue and incorrect predictions in red.\n","num_rows = 5\n","num_cols = 3\n","num_images = num_rows*num_cols\n","plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n","for i in range(num_images):\n","  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n","  plot_image(i, predictions[i], test_labels, test_images)\n","  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n","  plot_value_array(i, predictions[i], test_labels)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"9563131b-cc26-4da6-9302-8284c6a8bb5f","metadata":{"id":"9563131b-cc26-4da6-9302-8284c6a8bb5f"},"source":["### Usando el modelo entrenado\n","\n","Elijan la foto que más les guste y verifiquen si la red se equivoca o no."]},{"cell_type":"code","execution_count":null,"id":"2f1cefb6-43d9-40d0-a133-92aef67dacc2","metadata":{"id":"2f1cefb6-43d9-40d0-a133-92aef67dacc2"},"outputs":[],"source":["# Grab an image from the test dataset by modifiying the number in test_images\n","img = test_images[1]\n","\n","plt.figure()\n","plt.imshow(img)\n","plt.colorbar()\n","plt.grid(False)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"12de7cd3-f1e2-4b31-9975-f926e2f47c4c","metadata":{"id":"12de7cd3-f1e2-4b31-9975-f926e2f47c4c"},"outputs":[],"source":["# Add the image to a batch where it's the only member.\n","img = (np.expand_dims(img,0))\n","print(img.shape)"]},{"cell_type":"markdown","source":["Ejecuten la siguiente celda para ver si está en lo correcto."],"metadata":{"id":"Yc87Iu9vYzxM"},"id":"Yc87Iu9vYzxM"},{"cell_type":"code","execution_count":null,"id":"34d945f5-7fc8-4ba3-92ba-0c9ca2c97d7d","metadata":{"id":"34d945f5-7fc8-4ba3-92ba-0c9ca2c97d7d"},"outputs":[],"source":["predictions_single = probability_model.predict(img)\n","\n","plot_value_array(1, predictions_single[0], test_labels)\n","_ = plt.xticks(range(10), class_names, rotation=45)\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"fashion_mnist_classification.ipynb","provenance":[],"collapsed_sections":["zDXKczjhQuR2"],"private_outputs":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}