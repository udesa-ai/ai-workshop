{"cells":[{"cell_type":"markdown","metadata":{"id":"gZpHwHXrpYfU"},"source":["# License"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6T5bioQnUS_x"},"outputs":[],"source":["# Copyright 2021 University of San Andres' Authors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ubNHPpUUXjA"},"outputs":[],"source":["#@title MIT License\n","#\n","# Copyright (c) 2022 University of San Andres\n","#\n","# Permission is hereby granted, free of charge, to any person obtaining a\n","# copy of this software and associated documentation files (the \"Software\"),\n","# to deal in the Software without restriction, including without limitation\n","# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n","# and/or sell copies of the Software, and to permit persons to whom the\n","# Software is furnished to do so, subject to the following conditions:\n","#\n","# The above copyright notice and this permission notice shall be included in\n","# all copies or substantial portions of the Software.\n","#\n","# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n","# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n","# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n","# DEALINGS IN THE SOFTWARE."]},{"cell_type":"markdown","metadata":{"id":"Fs0EXaaRUiev"},"source":["# Diseña tu propia red neuronal: clasificación de flores\n","\n","![Tulip Field](https://hips.hearstapps.com/hbu.h-cdn.co/assets/17/09/1488567815-index-european-tulips.jpg)\n","\n","El objetivo de este práctico es definir nuestra propia red neuronal, es decir, la arquitectura que ésta debe tener de tal modo de conseguir una clasificación de flores lo más robusta posible.\n","\n","Los puntos a atacar son los siguientes:\n","\n","* Descargar el set de datos de imágenes de flores (3700 fotos).\n","* Visualizar las imágenes y los distintos tipos de flores (labels).\n","* Separar el set de datos en 3 subconjuntos de datos: `train`, `val`, `test`.\n","* Preprocesar los datos, una parte muy importante en algoritmos que aprenden.\n","* Definir arquitectura de la red neuronal.\n","* Ajustar hiperparámetros, por ejemplo, *learning rate*, *epochs*, etc.\n","* Entrenar y verificar cuán bien entrenamos.\n","* Aumento de datos para un mejor entrenamiento.\n","* Verificación final de los resultados.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uJYpfX0ajD1R"},"source":["## Primero los datos\n","\n","Ningún problema de Machine Learning (ML) puede ser atacado sin los datos, es por eso que en esta ocasión bajaremos imágenes de muchos tipos de flores de un set de datos públicos. Para esto utilizaremos *Python*, el lenguaje de programación más usado en aplicaciones de ML.\n","\n","La siguiente celda importará código disponible por desarrolladores y científicos en el mundo para que podamos descargar los datos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Egh6-s6RMd73"},"outputs":[],"source":["%load_ext tensorboard\n","\n","import PIL\n","import pathlib\n","import numpy as np\n","import tensorflow as tf\n","\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orcgJXEMrp5m"},"outputs":[],"source":["gpu_devices = tf.config.list_physical_devices('GPU')\n","for device in gpu_devices:\n","    tf.config.experimental.set_memory_growth(device, True)\n","\n","print('Num GPU available:', len(gpu_devices))"]},{"cell_type":"markdown","metadata":{"id":"DTvT064Xj88h"},"source":["Luego, procedemos a descargar los datos del sitio: https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n","\n","Con tan sólo ejecutar la siguiente celda, Python comenzará a descargar los datos por nosotros."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZXyv_PSHsl9"},"outputs":[],"source":["dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n","data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n","data_dir = pathlib.Path(data_dir)"]},{"cell_type":"markdown","metadata":{"id":"Dx5gkgDJkR9U"},"source":["Un paso importante es saber cuántas imágenes tenemos disponibles para poder trabajar. Ejecutando siguiente celda de código nos dirá exactamente cuantas fotos tenemos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2CHlJRAMZiO"},"outputs":[],"source":["image_count = len(list(data_dir.glob('*/*.jpg')))\n","print(image_count)"]},{"cell_type":"markdown","metadata":{"id":"Ljxom0exkiE4"},"source":["### Visualización de los datos\n","\n","Lo primero que vamos a hacer antes de comenzar a trabajar, es visualizar las fotos que tenemos. La siguientes celdas nos mostrarán una foto de cada tipo de flor que tenemos en el set de datos.\n","\n","Las clases de flores son:\n","* Rosas (*Roses*).\n","* Tulipanes (*Tulips*).\n","* Margaritas (*Daisies*).\n","* Diente de León (*Dandelion*).\n","* Girasol (*Sunflower*).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epwjLvsPMccd"},"outputs":[],"source":["rose = str(list(data_dir.glob('roses/*'))[0])\n","tulip = str(list(data_dir.glob('tulips/*'))[0])\n","daisy = str(list(data_dir.glob('daisy/*'))[0])\n","dandelion = str(list(data_dir.glob('dandelion/*'))[0])\n","sunflower = str(list(data_dir.glob('sunflowers/*'))[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqkmKoLilrwY"},"outputs":[],"source":["PIL.Image.open(rose)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmUJfxcXlvlJ"},"outputs":[],"source":["PIL.Image.open(tulip)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Be-sUPk6lv89"},"outputs":[],"source":["PIL.Image.open(daisy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHvJf0Helx2N"},"outputs":[],"source":["PIL.Image.open(sunflower)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_yH9emoly9k"},"outputs":[],"source":["PIL.Image.open(dandelion)"]},{"cell_type":"markdown","metadata":{"id":"c2jjrbVvnaE1"},"source":["## Separando los datos\n","\n","Tensorflow es una de las herramientas que nos ayudará con el todo el trabajo pesado de separación de datos en 3 distintos de subconjuntos: entrenamiento, validación, y prueba. En inglés, los llamaremos *training*, *validation*, y *testing*. También más adelante nos ayudará con la parte de entrenamiento.\n","\n","Antes de separar los datos debemos definir cuáles son los siguientes parámetros: `batch_size`, `img_height`, `img_width`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3TsWDCaPZNV"},"outputs":[],"source":["batch_size =\n","img_height = \n","img_width = "]},{"cell_type":"markdown","metadata":{"id":"3ah6RCD08gR-"},"source":["### Set de entrenamiento\n","\n","La siguiente celda se encarga de generar, a partir del set de datos completo, el set de datos de entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oe9zgBXRPbHT"},"outputs":[],"source":["train_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"GUvop33h8qQw"},"source":["### Set de validación\n","\n","Esta celda, genera el set de datos de validación."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHcS93bwPsbD"},"outputs":[],"source":["val_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"aY8LBnM0844k"},"source":["### Clases de flores\n","\n","Tal y como anunciamos anteriormente, Tensorflow conoce las clases que hay en el set de datos. Para ello ejecutamos el código de la siguiente celda.\n","\n","**IMPORTANTE**: notar que la persona que armó el set de datos en Tensorflow ya había definido la clases en el set de datos, de tal manera que sea posible el entrenamiento con los algoritmos de ML."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c88UsUk_PwgX"},"outputs":[],"source":["class_names = train_ds.class_names\n","print(class_names)"]},{"cell_type":"markdown","metadata":{"id":"lN2UvqOB9vkN"},"source":["También podemos visualizar nuevamente las imágenes con sus respectivas etiquetas utilizando otra herramiento llamada `matplotlib`. Ejecutamos la siguiente celda para ver las fotos de una manera distinta a la anterior, pero esta vez sólo del set de datos de entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CA_E1SGUPzIp"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 10))\n","for images, labels in train_ds.take(1):\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i].numpy().astype(\"uint8\"))\n","    plt.title(class_names[labels[i]])\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"yczDAzGf-5kq"},"source":["## Preprocesando los datos\n","\n","Para ingresar los datos a la red neuronal es necesario aplicar un tipo de procesamiento previo. Antes, al cargar los set de datos de entrenamien y validación, ya aplicamos algunas transformaciones a los datos. En primer lugar, elegimos qué tamaño tengan las imágenes; porque no todas las imágenes originales tienen las mismas dimensiones. En segundo lugar, exigimos que los datos vengan de a grupos de 32. Dicho en palabras, nuestros datos vendrán en grupos de 32, donde cada canal de la imagen tendrá un ancho por alto de 180 pixels por 180 pixels. Cada canal corresponde al canal rojo, azul y verde de una foto a color.\n","\n","Al ejecutar la siguiente celda vamos a ver del primer grupo de imágenes para ingresar a la red neuronal."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqr9hDlIP5ux"},"outputs":[],"source":["for image_batch, labels_batch in train_ds:\n","  print(f\"Image batch size: {image_batch.shape}\")\n","  print(f\"Label batch size: {labels_batch.shape}\")\n","  break"]},{"cell_type":"markdown","metadata":{"id":"QOzLpm6JAe1R"},"source":["### Normalización\n","\n","Lo próximo que tenemos que hacer es normalizar las imágenes antes de ingresar a la red neuronal. Normalizar es el proceso por el cual los valores de los pixeles de una imagen van a ir entre valores del 0 al 1.\n","\n","Para demostrar esto, visualizaremos los siguiente. Primero, vamos a mostrar los valores de los pixels de una imagen. Y luego, vamos a ver que el máximo valor de la imagen normalizada es 1 y el mínimo es 0; en el medio pueden haber valores entre el 0 y el 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEO3e0YSQLrk"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"aYV7bj4mD_Wd"},"source":["En este primer caso, visualizamos los valores de cada pixel y vemos que no corresponden entre 0 y 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ladkn7EPDrxc"},"outputs":[],"source":["image_batch, labels_batch = next(iter(train_ds))\n","image_batch[0]"]},{"cell_type":"markdown","metadata":{"id":"Jy9oZF2FEa9p"},"source":["Aplicamos la normalización a esta misma imagen, y volvemos a mostrar los valores de los pixels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5jQyCyZQSMR"},"outputs":[],"source":["normalization_layer = tf.keras.layers.Rescaling(1./255)\n","normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n","image_batch, labels_batch = next(iter(normalized_ds))\n","image_batch[0]\n"]},{"cell_type":"markdown","metadata":{"id":"jdO7_jutEjwk"},"source":["Y ahora verificamos que el máximo de la imagen sea 1 y el mínimo 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BzWTWtRiEnCj"},"outputs":[],"source":["# Notice the pixel values are now in `[0,1]`.\n","print(f\"Mínimo: {np.min(image_batch[0])}\", f\"Máximo: {np.max(image_batch[0])}\")"]},{"cell_type":"markdown","metadata":{"id":"fz_3yxA1FAzW"},"source":["## Diseñando la red neuronal\n","\n","A partir de ahora estamos listos para definir y crear la red neuronal, es decir, explicitar la arquitectura que va a tener nuestra red.\n","\n","Para ello tenemos la opción de agregar tipos de capas. Recordar que no son todas obligatorias y no hay límite de cuántas agregar (en principio).\n","\n","Tipos de capas:\n","* `tf.keras.layers.Conv2D(filters, kernel_size, padding='same', activation=None)`\n","* `tf.keras.layers.MaxPooling2D()`\n","* `tf.keras.layers.Flatten()`\n","* `tf.keras.layers.Dropout(rate)`\n","* `tf.keras.layers.Dense(units, activation=None)`\n","\n","La idea es que ustedes investiguen y jueguen de tal manera de que se pueda armar una red capaz de poder entrenarse. Es importante empezar de a pasos chiquitos e ir creciendo la red hasta que mejore.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIc-yLRmQYp2"},"outputs":[],"source":["num_classes = len(class_names)\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n","  # Complete architecture\n","  tf.keras.layers.Dense(num_classes)\n","])"]},{"cell_type":"markdown","metadata":{"id":"WLPieNy0KbPs"},"source":["Una vez definido la arquitectura, debemos elegir algunos parámetros más. En este caso no es necesario modificar nada, les damos los parámetros de compilación."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INMQn4Y3Qhg3"},"outputs":[],"source":["# Choose your learning rate!\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=['accuracy'],\n",")"]},{"cell_type":"markdown","metadata":{"id":"hlcxjn8LKmiF"},"source":["Con la siguiente celda podemos ver el resúmen de la red neuronal. Finalmente, nos va a decir cuántos parámetros van a ser necesario entrenar con la red que definimos. Tengan cuidado que pueden ser MUCHOS!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_3QeekCQjwp"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"PIDWlwqWr-J0"},"source":["### Tensorboard\n","\n","Tensorboard nos hará el trabajo más divertido, ya que nos permitirá tener una visualización del trabajo que está realizando la red neuronal, y también, nos dirá cómo le está yendo en términos de desempeño. Ejecutamos la próxima celda para inicializar Tensorboard."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLZnA4nEr8fL"},"outputs":[],"source":["import os\n","import datetime\n","\n","basedir = '/tmp/flowers/'\n","logdir = os.path.join(basedir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","os.makedirs(logdir, exist_ok=True)\n","\n","%tensorboard --reload_multifile True --logdir {basedir}"]},{"cell_type":"markdown","metadata":{"id":"j_BvYurALqIw"},"source":["## Entrenamiento\n","\n","Ahora vamos a comenzar a entrenar el algoritmo. Recuerden elegir la cantidad de epochs que deseen entrenar."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EO5DrHaSQmU3"},"outputs":[],"source":["epochs = 0 # Choose your number of epochs. Just try!\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=10,\n","  callbacks=[tensorboard_callback],\n",")"]},{"cell_type":"markdown","metadata":{"id":"UAJO5fsOuA58"},"source":["### Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNz1wFq-uED1"},"outputs":[],"source":["def test(flower_url, filename):\n","  path = tf.keras.utils.get_file(filename, origin=flower_url)\n","\n","  img = tf.keras.utils.load_img(\n","      path, target_size=(img_height, img_width)\n","  )\n","  img_array = tf.keras.utils.img_to_array(img)\n","  img_array = tf.expand_dims(img_array, 0) # Create a batch\n","\n","  predictions = model.predict(img_array)\n","  score = tf.nn.softmax(predictions[0])\n","\n","  print(\n","      \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n","      .format(class_names[np.argmax(score)], 100 * np.max(score))\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWdnzC9PuRXy"},"outputs":[],"source":["test(\"https://storage.googleapis.com/ds-workshop/flowers/test/roses/3278995478.jpg\", \"3278995478\")"]},{"cell_type":"markdown","metadata":{"id":"ioT6uHZeLo7f"},"source":["¡Felicitaciones! Entrenaste la red neuronal, pero tengo que mostrate algo, un problema muy común en machine learning.\n"]},{"cell_type":"markdown","metadata":{"id":"vEyEtTtWMQH1"},"source":["## Overfitting\n","\n","El *overfitting* es un problema extremadamente común en los algoritmos de aprendizaje y también es muy intuitivo de cómo aprenden los sistemas en general. Para revisar que nuestra red neuronal comenzó a sufrir de este problema, es necesario graficar la *accuracy* de entrenamiento y la *accuracy* de validación. Ejecutemos la siguiente celda y veamos..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5kRU10sQmab"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nhUlaCuzMuGi"},"source":["Lo que estamos viendo es que la accuracy de entrenamiento es muchísimo más alta que la accuracy de validación. Recuerden que la red neuronal se entrena con los datos de entrenamiento, y los datos de validación se utilizan durante el entrenamiento para verificar cómo le está yendo.\n","\n","Vemos que la red neuronal aprendió \"de memoria\" los datos de entrenamiento, y cuando le mostramos datos que nunca antes había visto, no supo clasificar bien los datos. Este es el problema de overfitting."]},{"cell_type":"markdown","metadata":{"id":"mNOhEsDWXMw0"},"source":["## Data Augmentation\n","\n","*Data augmentation* es una técnica por la cual podemos ayudar a solucionar el overfitting. La idea es aplicar transformaciones, que tengan sentido, a los datos para generar nuevos datos. Por ejemplo, el caso más común es el de espejar una imagen que puede ayudar a los algortimos a aprender mejor. Nosotros, los humanos, sabemos que una foto de un gato espejada, sigue teniendo un gato y podemos encima decir qué gato es. Entonces, ¿por qué una red neuronal no podría hacer lo mismo?\n","\n","Veamos cómo Tensorflow nos ayuda a aumentar los datos."]},{"cell_type":"markdown","metadata":{"id":"6D3Ye0XCyhoA"},"source":["### Transformaciones\n","\n","Las transformaciones que aplicaremos son:\n","\n","* Espejar horizontalmente la imagen.\n","* Rotar de forma aleatoria un factor del número `π`.\n","* Un zoom aleatorio de un factor dado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EzzPfngxY3ZC"},"outputs":[],"source":["data_augmentation = tf.keras.models.Sequential(\n","  [\n","    tf.keras.layers.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3)),\n","    tf.keras.layers.RandomRotation(0.1),\n","    tf.keras.layers.RandomZoom(0.1),\n","  ]\n",")"]},{"cell_type":"markdown","metadata":{"id":"AmofR0NTzBh-"},"source":["Ahora visualizamos cómo se ve una misma imagen con distintas transformaciones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73HrLUp_Z-db"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","for images, _ in train_ds.take(1):\n","  for i in range(9):\n","    augmented_images = data_augmentation(images)\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"yE6rzFR5zNtk"},"source":["Ya que sabemos que cómo se ven las transformaciones, vamos a sumarselas al modelo para que pueda se entrenado con ese tipo de datos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwpubmDYaE4n"},"outputs":[],"source":["model = tf.keras.models.Sequential([\n","  data_augmentation, # We add the augmentation\n","  tf.keras.layers.Rescaling(1./255),\n","  # Put the same layers as before.\n","  tf.keras.layers.Dense(num_classes)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRYZOSViah5N"},"outputs":[],"source":["# Again, choose your learning rate.\n","\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JBmuIgIak03"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"o326r2xdzUx-"},"source":["Elegimos nuevamente la cantidad de epochs que queremos entrenar...y esperamos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZlZ0XgJan4U"},"outputs":[],"source":["epochs = 0 # Don't forget to set the epochs\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"markdown","metadata":{"id":"pTw6BiRxzcbC"},"source":["Ejecutamos la celda, y vemos cómo se entreno. O también podemos ir a Tensorboard más arriba a visualizar la accuracy y la loss function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvgTVIKKau2z"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"m_XhXs8fzlBd"},"source":["¡Hubo una gran mejora en cuanto al overfitting! Ya la accuracies están más cercanas durante todo el entrenamiento.\n","\n","Ahora probemos si clasifica bien."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BgoUWcvFbMmS"},"outputs":[],"source":["test(\"https://storage.googleapis.com/ds-workshop/flowers/test/tulips/2208869753.jpg\", \"2208869753\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNRIULqrvuVvIxpNBwZ8ly6","collapsed_sections":["gZpHwHXrpYfU"],"name":"flowers.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
